{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 2. Entendimiento y Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección tiene como propósito exponer el entendimiento y la preparación de los textos para posteriormente utilizarlos en la implementación de los modelos de aprendizaje. Para la manipulación de los datos se usarán librerias como **pandas** y **numpy**. Para la visualización se utilizarán **matplotlib**, **seaborn** y **WordCloud**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación e importación de librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas numpy matplotlib seaborn scikit-learn nltk spacy wordcloud tqdm\n",
    "#%pip install spacy-langdetect\n",
    "#!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mgs05\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Procesamiento de texto\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Modelado y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Progreso en bucles\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Descargar stopwords de NLTK en español\n",
    "nltk.download('stopwords')\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perfilamiento y entendimiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "data=pd.read_csv('fake_news_spanish.csv', sep=';', encoding = \"utf-8\")\n",
    "\n",
    "news_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
       "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57063 entries, 0 to 57062\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           57063 non-null  object\n",
      " 1   Label        57063 non-null  int64 \n",
      " 2   Titulo       57047 non-null  object\n",
      " 3   Descripcion  57063 non-null  object\n",
      " 4   Fecha        57063 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Limpieza de datos.\n",
    "\n",
    "Este código implementa un proceso de preprocesamiento de texto para limpiar datos textuales antes de realizar análisis de texto o procesamiento de lenguaje natural.El preprocesamiento se aplica a dos columnas de un DataFrame (Titulo y Descripcion), generando nuevas columnas con los textos limpios (Titulo_Limpio y Descripcion_Limpia). Este paso es fundamental porque ayuda a reducir el ruido en los datos, mejora la precisión de los modelos y facilita la extracción de información relevante eliminando términos irrelevantes o redundantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Comparación antes y después de la limpieza:\n",
      "                                              Titulo  \\\n",
      "0  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
      "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
      "2  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
      "3  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
      "4  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
      "\n",
      "                                       Titulo_Limpio  \n",
      "0  the guardian va sanchez europa necesita apuest...  \n",
      "1  revelan gobierno negocio liberacion mireles ca...  \n",
      "2  ahora nunca joan fuster estatuto valenciano cu...  \n",
      "3  iglesias alienta yolanda diaz erc eh bildu neg...  \n",
      "4  puigdemont seria ninguna tragedia repeticion e...  \n",
      "                                         Descripcion  \\\n",
      "0  El diario británico publicó este pasado jueves...   \n",
      "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
      "2  El valencianismo convoca en Castelló su fiesta...   \n",
      "3  En política, igual que hay que negociar con lo...   \n",
      "4  En una entrevista en El Punt Avui, el líder de...   \n",
      "\n",
      "                                  Descripcion_Limpia  \n",
      "0  diario britanico publico pasado jueves editori...  \n",
      "1  revelan gobierno negocio liberacion mireles ca...  \n",
      "2  valencianismo convoca castello fiesta grande c...  \n",
      "3  politica igual negociar empresarios negociar g...  \n",
      "4  entrevista punt avui lider jxcat desdramatizad...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Funciones de preprocesamiento\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remueve caracteres no ASCII de una lista de palabras\"\"\"\n",
    "    return [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words if word]\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convierte todas las palabras a minúsculas\"\"\"\n",
    "    return [word.lower() for word in words if word]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Elimina signos de puntuación\"\"\"\n",
    "    return [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Elimina los números de la lista de palabras\"\"\"\n",
    "    return [word for word in words if not word.isdigit()]\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Elimina stopwords en español\"\"\"\n",
    "    return [word for word in words if word not in stopwords_es]\n",
    "\n",
    "def preprocessing(text):\n",
    "    \"\"\"Aplica todas las funciones de limpieza de texto a un string\"\"\"\n",
    "    words = word_tokenize(text, language=\"spanish\")  # Tokenizar con NLTK\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return \" \".join(words)  # Retorna el texto limpio como un string\n",
    "\n",
    "# Aplicar limpieza en el DataFrame\n",
    "news_df['Titulo_Limpio'] = news_df['Titulo'].astype(str).apply(preprocessing)\n",
    "news_df['Descripcion_Limpia'] = news_df['Descripcion'].astype(str).apply(preprocessing)\n",
    "\n",
    "# Comparación antes y después\n",
    "print(\"🔹 Comparación antes y después de la limpieza:\")\n",
    "print(news_df[['Titulo', 'Titulo_Limpio']].head())\n",
    "print(news_df[['Descripcion', 'Descripcion_Limpia']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tokenización\n",
    "\n",
    "Este código realiza la tokenización de texto. Esta tokenización se aplica a las columnas ya limpias del DataFrame (Titulo_Limpio y Descripcion_Limpia), generando nuevas columnas Titulo_Tokens y Descripcion_Tokens, donde cada texto se representa como una lista de palabras. Este paso es crucial porque permite a los modelos de análisis de texto procesar los datos de manera estructurada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación después de la tokenización:\n",
      "                                       Titulo_Limpio  \\\n",
      "0  the guardian va sanchez europa necesita apuest...   \n",
      "1  revelan gobierno negocio liberacion mireles ca...   \n",
      "2  ahora nunca joan fuster estatuto valenciano cu...   \n",
      "3  iglesias alienta yolanda diaz erc eh bildu neg...   \n",
      "4  puigdemont seria ninguna tragedia repeticion e...   \n",
      "\n",
      "                                       Titulo_Tokens  \n",
      "0  [the, guardian, va, sanchez, europa, necesita,...  \n",
      "1  [revelan, gobierno, negocio, liberacion, mirel...  \n",
      "2  [ahora, nunca, joan, fuster, estatuto, valenci...  \n",
      "3  [iglesias, alienta, yolanda, diaz, erc, eh, bi...  \n",
      "4  [puigdemont, seria, ninguna, tragedia, repetic...  \n",
      "                                  Descripcion_Limpia  \\\n",
      "0  diario britanico publico pasado jueves editori...   \n",
      "1  revelan gobierno negocio liberacion mireles ca...   \n",
      "2  valencianismo convoca castello fiesta grande c...   \n",
      "3  politica igual negociar empresarios negociar g...   \n",
      "4  entrevista punt avui lider jxcat desdramatizad...   \n",
      "\n",
      "                                  Descripcion_Tokens  \n",
      "0  [diario, britanico, publico, pasado, jueves, e...  \n",
      "1  [revelan, gobierno, negocio, liberacion, mirel...  \n",
      "2  [valencianismo, convoca, castello, fiesta, gra...  \n",
      "3  [politica, igual, negociar, empresarios, negoc...  \n",
      "4  [entrevista, punt, avui, lider, jxcat, desdram...  \n"
     ]
    }
   ],
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text, language=\"spanish\")\n",
    "\n",
    "# Aplicar tokenización en las columnas ya limpias\n",
    "news_df['Titulo_Tokens'] = news_df['Titulo_Limpio'].astype(str).apply(tokenize_nltk)\n",
    "news_df['Descripcion_Tokens'] = news_df['Descripcion_Limpia'].astype(str).apply(tokenize_nltk)\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"Comparación después de la tokenización:\")\n",
    "print(news_df[['Titulo_Limpio', 'Titulo_Tokens']].head())\n",
    "print(news_df[['Descripcion_Limpia', 'Descripcion_Tokens']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Normalización\n",
    "\n",
    "Este código realiza la normalización del texto aplicando técnicas de stemming y lemmatización. La normalización se aplica a las columnas del DataFrame (Titulo_Tokens y Descripcion_Tokens), generando nuevas columnas Titulo_Normalizado y Descripcion_Normalizada, donde cada palabra es reducida a su raíz. Este paso es fundamental porque mejora la consistencia del texto y facilita el procesamiento de datos en modelos de análisis de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mgs05\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mgs05\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación después de la normalización:\n",
      "                                       Titulo_Tokens  \\\n",
      "0  [the, guardian, va, sanchez, europa, necesita,...   \n",
      "1  [revelan, gobierno, negocio, liberacion, mirel...   \n",
      "2  [ahora, nunca, joan, fuster, estatuto, valenci...   \n",
      "3  [iglesias, alienta, yolanda, diaz, erc, eh, bi...   \n",
      "4  [puigdemont, seria, ninguna, tragedia, repetic...   \n",
      "\n",
      "                                  Titulo_Normalizado  \n",
      "0  [guardian, sanchez, va, guardi, apuest, europ,...  \n",
      "1  [cambio, negoci, javier, duart, gobiern, javi,...  \n",
      "2  [fuster, anos, valenciano, cumple, nunc, ahor,...  \n",
      "3  [negoci, iglesias, investidura, alient, bloqu,...  \n",
      "4  [eleccion, elecciones, ninguna, ningun, seri, ...  \n",
      "                                  Descripcion_Tokens  \\\n",
      "0  [diario, britanico, publico, pasado, jueves, e...   \n",
      "1  [revelan, gobierno, negocio, liberacion, mirel...   \n",
      "2  [valencianismo, convoca, castello, fiesta, gra...   \n",
      "3  [politica, igual, negociar, empresarios, negoc...   \n",
      "4  [entrevista, punt, avui, lider, jxcat, desdram...   \n",
      "\n",
      "                             Descripcion_Normalizada  \n",
      "0  [eleccion, president, apoya, riesg, presidente...  \n",
      "1  [preferencial, par, duart, turbios, liberarlo,...  \n",
      "2  [fiesta, batalla, castell, plaza, castello, co...  \n",
      "3  [parlamentari, negoci, parlamentarios, politic...  \n",
      "4  [acabe, eleccion, escenari, cup, trab, teo, ga...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Descargar recursos necesarios para lematización\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Crear los objetos para stemming y lematización\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Aplica stemming para eliminar prefijos y sufijos en las palabras\"\"\"\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Aplica lematización para obtener la raíz de los verbos\"\"\"\n",
    "    return [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    \"\"\"Combina stemming y lematización para normalizar los datos\"\"\"\n",
    "    stemmed = stem_words(words)\n",
    "    lemmatized = lemmatize_verbs(words)\n",
    "    return list(set(stemmed + lemmatized))  # Se usa set() para evitar duplicados\n",
    "\n",
    "# Aplicar normalización a las columnas tokenizadas\n",
    "news_df['Titulo_Normalizado'] = news_df['Titulo_Tokens'].apply(stem_and_lemmatize)\n",
    "news_df['Descripcion_Normalizada'] = news_df['Descripcion_Tokens'].apply(stem_and_lemmatize)\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"Comparación después de la normalización:\")\n",
    "print(news_df[['Titulo_Tokens', 'Titulo_Normalizado']].head())\n",
    "print(news_df[['Descripcion_Tokens', 'Descripcion_Normalizada']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
