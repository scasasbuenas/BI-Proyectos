{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 2. Entendimiento y Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección tiene como propósito exponer el entendimiento y la preparación de los textos para posteriormente utilizarlos en la implementación de los modelos de aprendizaje. Para la manipulación de los datos se usarán librerias como **pandas** y **numpy**. Para la visualización se utilizarán **matplotlib**, **seaborn** y **WordCloud**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación e importación de librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas numpy matplotlib seaborn scikit-learn nltk spacy wordcloud tqdm\n",
    "#%pip install spacy-langdetect\n",
    "#!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Procesamiento de texto\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Modelado y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Progreso en bucles\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Descargar stopwords de NLTK en español\n",
    "nltk.download('stopwords')\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perfilamiento y entendimiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "data=pd.read_csv('fake_news_spanish.csv', sep=';', encoding = \"utf-8\")\n",
    "\n",
    "news_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
       "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57063 entries, 0 to 57062\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           57063 non-null  object\n",
      " 1   Label        57063 non-null  int64 \n",
      " 2   Titulo       57047 non-null  object\n",
      " 3   Descripcion  57063 non-null  object\n",
      " 4   Fecha        57063 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Limpieza de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remueve caracteres no ASCII de una lista de palabras\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convierte todas las palabras a minúsculas\"\"\"\n",
    "    return [word.lower() for word in words if word is not None]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Elimina signos de puntuación\"\"\"\n",
    "    return [re.sub(r'[^\\w\\s]', '', word) for word in words if word is not None and re.sub(r'[^\\w\\s]', '', word) != '']\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Elimina los números de la lista de palabras\"\"\"\n",
    "    return [word for word in words if not word.isdigit()]\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Elimina stopwords en español\"\"\"\n",
    "    return [word for word in words if word not in stopwords_es]\n",
    "\n",
    "def preprocessing(text):\n",
    "    \"\"\"Aplica todas las funciones de limpieza de texto a un string\"\"\"\n",
    "    words = text.split()  # Tokenización básica separando por espacios\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return \" \".join(words)  # Retorna el texto limpio como un string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Titulo  \\\n",
      "0  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
      "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
      "2  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
      "3  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
      "4  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
      "\n",
      "                                       Titulo_Limpio  \n",
      "0  the guardian va sanchez europa necesita apuest...  \n",
      "1  revelan gobierno negocio liberacion mireles ca...  \n",
      "2  ahora nunca joan fuster estatuto valenciano cu...  \n",
      "3  iglesias alienta yolanda diaz erc eh bildu neg...  \n",
      "4  puigdemont seria ninguna tragedia repeticion e...  \n",
      "                                         Descripcion  \\\n",
      "0  El diario británico publicó este pasado jueves...   \n",
      "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
      "2  El valencianismo convoca en Castelló su fiesta...   \n",
      "3  En política, igual que hay que negociar con lo...   \n",
      "4  En una entrevista en El Punt Avui, el líder de...   \n",
      "\n",
      "                                  Descripcion_Limpia  \n",
      "0  diario britanico publico pasado jueves editori...  \n",
      "1  revelan gobierno negocio liberacion mireles ca...  \n",
      "2  valencianismo convoca castello fiesta grande c...  \n",
      "3  politica igual negociar empresarios negociar g...  \n",
      "4  entrevista punt avui lider jxcat desdramatizad...  \n"
     ]
    }
   ],
   "source": [
    "# Aplicar la limpieza a la columna 'Titulo' y 'Descripcion'\n",
    "news_df['Titulo_Limpio'] = news_df['Titulo'].astype(str).apply(preprocessing)\n",
    "news_df['Descripcion_Limpia'] = news_df['Descripcion'].astype(str).apply(preprocessing)\n",
    "\n",
    "# Compración antes y después de la limpieza\n",
    "print(news_df[['Titulo', 'Titulo_Limpio']].head())\n",
    "print(news_df[['Descripcion', 'Descripcion_Limpia']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Descarga el tokenizer de NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Titulo  \\\n",
      "0  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
      "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
      "2  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
      "3  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
      "4  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
      "\n",
      "                                       Titulo_Tokens  \n",
      "0  ['The, Guardian, ', va, con, Sánchez, :, 'Euro...  \n",
      "1  [REVELAN, QUE, EL, GOBIERNO, NEGOCIO, LA, LIBE...  \n",
      "2  [El, 'Ahora, o, nunca, ', de, Joan, Fuster, so...  \n",
      "3  [Iglesias, alienta, a, Yolanda, Díaz, ,, ERC, ...  \n",
      "4  [Puigdemont, :, 'No, sería, ninguna, tragedia,...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')  # Descarga el tokenizer de NLTK\n",
    "def tokenize_nltk(text):\n",
    "    \"\"\"Tokeniza el texto usando NLTK\"\"\"\n",
    "    return word_tokenize(text, language=\"spanish\")  # Especificamos español\n",
    "\n",
    "# Aplicar tokenización en el DataFrame\n",
    "news_df['Titulo_Tokens'] = news_df['Titulo'].astype(str).apply(tokenize_nltk)\n",
    "news_df['Descripcion_Tokens'] = news_df['Descripcion'].astype(str).apply(tokenize_nltk)\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(news_df[['Titulo', 'Titulo_Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
